# Data Engineering ETL Assignment

> **End-to-end ETL pipeline using Azure Data Factory, Databricks, Azure SQL Database, and Azure Table Storage**

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

## ğŸ“‹ Table of Contents

- [Overview](#overview)
- [Architecture](#architecture)
- [Technologies Used](#technologies-used)
- [Data Sources](#data-sources)
- [Pipeline Flow](#pipeline-flow)
- [Setup Instructions](#setup-instructions)
- [Running the Pipeline](#running-the-pipeline)
- [Design Decisions](#design-decisions)
- [Data Quality](#data-quality)
- [SQL Transformations](#sql-transformations)
- [Assumptions & Limitations](#assumptions--limitations)
- [Sample Outputs](#sample-outputs)
- [Project Structure](#project-structure)

---

## ğŸ¯ Overview

This project demonstrates a **production-grade ETL pipeline** that:

- **Ingests** data from multiple sources (CSV, JSON)
- **Transforms** data through Bronze â†’ Silver â†’ Gold layers (Medallion Architecture)
- **Loads** to appropriate data stores (SQL Database for analytics, Table Storage for operational queries)
- Implements **incremental loading**, **data quality checks**, and **idempotent operations**

### Key Features

âœ… **Separate pipelines** for transactional vs event data  
âœ… **Medallion architecture** (Bronze/Silver/Gold layers)  
âœ… **SQL transformations** with window functions and complex joins  
âœ… **Star schema** design for dimensional modeling  
âœ… **NoSQL storage** for semi-structured event data  
âœ… **Data quality** validation and error handling  
âœ… **Incremental loads** with watermark pattern  
âœ… **Idempotent** pipeline design (safe to rerun)

---

## ğŸ—ï¸ Architecture

### High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Data Sources  â”‚         â”‚  Orchestration   â”‚         â”‚  Target Stores  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                 â”‚         â”‚                  â”‚         â”‚                 â”‚
â”‚  CSV Files      â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  Azure Data      â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  Azure SQL DB   â”‚
â”‚  (Sales)        â”‚         â”‚  Factory         â”‚         â”‚  (Star Schema)  â”‚
â”‚                 â”‚         â”‚                  â”‚         â”‚                 â”‚
â”‚  JSON Files     â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  +               â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  Azure Table    â”‚
â”‚  (Events)       â”‚         â”‚                  â”‚         â”‚  Storage        â”‚
â”‚                 â”‚         â”‚  Azure           â”‚         â”‚  (Key-Value)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚  Databricks      â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚  (Spark SQL)     â”‚
                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚
                                     â–¼
                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                            â”‚  Azure Data Lake â”‚
                            â”‚  Gen2 (Storage)  â”‚
                            â”‚                  â”‚
                            â”‚  Bronze Layer    â”‚
                            â”‚  Silver Layer    â”‚
                            â”‚  Gold Layer      â”‚
                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Detailed Pipeline Flow

```
Sales Pipeline (Batch - Daily):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
CSV Files â†’ ADF Ingest â†’ Raw â†’ Bronze â†’ Databricks Clean â†’ Silver 
  â†’ Databricks Transform â†’ Gold â†’ SQL Database (Star Schema)

Events Pipeline (Micro-batch - Hourly):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
JSON Files â†’ ADF Ingest â†’ Raw â†’ Bronze â†’ Databricks Parse â†’ Silver 
  â†’ Databricks Sessionize â†’ Gold â†’ Table Storage (Documents)
```

See [Architecture Diagram](docs/architecture_diagram.png) for visual representation.

---

## ğŸ› ï¸ Technologies Used

| Component | Technology | Purpose |
|-----------|-----------|---------|
| **Orchestration** | Azure Data Factory | Pipeline scheduling, data movement |
| **Transformation** | Azure Databricks (PySpark) | Data cleaning, business logic, SQL |
| **Relational Storage** | Azure SQL Database | Star schema, analytical queries |
| **NoSQL Storage** | Azure Table Storage | Event logs, high-volume key-value |
| **Data Lake** | Azure Data Lake Gen2 | Medallion architecture (Bronze/Silver/Gold) |
| **Languages** | Python, Spark SQL, T-SQL | Data processing and transformations |

---

## ğŸ“Š Data Sources

### 1. Transactional Data (CSV)

**File**: `sales_orders.csv`  
**Volume**: ~10,000 orders  
**Schema**:
```
order_id, customer_id, product_id, order_date, order_timestamp,
quantity, unit_price, discount_amount, shipping_cost, line_total,
status, payment_method, last_modified
```

**Sample**:
```csv
order_id,customer_id,product_id,order_date,quantity,unit_price,status
ORD000001,CUST0042,PROD003,2024-01-15,2,19.99,completed
ORD000002,CUST0123,PROD001,2024-01-15,1,1299.99,completed
```

### 2. Event/Log Data (JSON)

**File**: `user_events.json`  
**Volume**: ~50,000 events  
**Format**: Newline-delimited JSON

**Sample**:
```json
{
  "event_id": "EVT00000001",
  "user_id": "USER00123",
  "event_type": "page_view",
  "timestamp": "2024-01-15T10:30:45Z",
  "session_id": "SESS123456",
  "metadata": {
    "page": "/products/electronics",
    "device": "mobile",
    "browser": "Chrome"
  }
}
```

### Reference Data

- **Customers**: 200 customers with segments (Premium/Standard/Budget)
- **Products**: 15 products across Electronics, Furniture, Office Supplies

---

## ğŸ”„ Pipeline Flow

### Phase 1: Ingestion (ADF)

1. **Trigger**: Scheduled (Sales: daily 2 AM, Events: hourly)
2. **Watermark Check**: Query audit table for last processed timestamp
3. **Copy Activity**: Read source files â†’ Write to Raw/Bronze layers
4. **Incremental Logic**: Only load records newer than watermark

### Phase 2: Transformation (Databricks)

**Notebook 1: Bronze â†’ Silver (Data Quality)**
- Remove duplicates
- Validate critical fields
- Standardize formats (UPPER, TRIM, type casting)
- Filter invalid records â†’ Quarantine
- Write clean data to Silver

**Notebook 2: Silver â†’ Gold (Business Logic)**
- Join sales with customer/product dimensions
- Calculate profit = revenue - cost
- Create star schema (dims + facts)
- Generate aggregates (customer/product metrics)
- Sessionize events (group by session_id)

**Notebook 3: Advanced Analytics**
- Window functions (ROW_NUMBER, RANK, moving averages)
- Customer lifetime value
- Product performance rankings
- Conversion funnel analysis

### Phase 3: Loading (ADF + Databricks)

1. **Databricks** writes to Gold layer (Parquet)
2. **ADF** triggers load pipeline
3. **SQL Database**: MERGE into dimensions and facts (idempotent)
4. **Table Storage**: Bulk insert session documents
5. **Update watermarks** in audit table

---

## ğŸš€ Setup Instructions

### Prerequisites

- Azure subscription (or free tier)
- Azure Data Factory instance
- Azure Databricks workspace (cluster)
- Azure SQL Database
- Azure Storage Account (Data Lake Gen2 enabled)
- Azure Table Storage

### Step 1: Clone Repository

```bash
git clone https://github.com/yourusername/data-engineering-etl-assignment.git
cd data-engineering-etl-assignment
```

### Step 2: Generate Sample Data

```bash
cd sample_data
python generate_sales_data.py
python generate_event_data.py
```

This creates:
- `sales_orders.csv` (10,000+ orders)
- `customers.csv` (200 customers)
- `products.csv` (15 products)
- `user_events.json` (50,000+ events)

### Step 3: Setup Azure Resources

#### 3.1 Create Resource Group

```bash
az group create --name rg-data-engineering --location eastus
```

#### 3.2 Create Storage Account (Data Lake)

```bash
az storage account create \
  --name dlsyourstorage \
  --resource-group rg-data-engineering \
  --location eastus \
  --sku Standard_LRS \
  --kind StorageV2 \
  --hierarchical-namespace true
```

Create containers: `raw`, `bronze`, `silver`, `gold`

#### 3.3 Create Azure SQL Database

```bash
az sql server create \
  --name sql-dataeng-server \
  --resource-group rg-data-engineering \
  --location eastus \
  --admin-user sqladmin \
  --admin-password YourPassword123!

az sql db create \
  --name sales_analytics_db \
  --server sql-dataeng-server \
  --resource-group rg-data-engineering \
  --service-objective S0
```

Run DDL script:
```bash
sqlcmd -S sql-dataeng-server.database.windows.net -d sales_analytics_db \
  -U sqladmin -P YourPassword123! -i sql/01_ddl_schema.sql
```

#### 3.4 Create Azure Table Storage

Already included in the storage account created above.

#### 3.5 Create Data Factory

```bash
az datafactory create \
  --name adf-data-engineering \
  --resource-group rg-data-engineering \
  --location eastus
```

#### 3.6 Create Databricks Workspace

```bash
az databricks workspace create \
  --name dbw-data-engineering \
  --resource-group rg-data-engineering \
  --location eastus \
  --sku premium
```

### Step 4: Upload Sample Data

```bash
# Upload to storage account
az storage blob upload-batch \
  --account-name dlsyourstorage \
  --destination raw/sales \
  --source sample_data/*.csv

az storage blob upload-batch \
  --account-name dlsyourstorage \
  --destination raw/events \
  --source sample_data/*.json
```

### Step 5: Import ADF Pipelines

1. Open Azure Data Factory Studio
2. Go to Author â†’ Pipelines â†’ Import
3. Upload JSON files from `/adf/` folder
4. Update linked services with your connection strings

### Step 6: Import Databricks Notebooks

1. Open Databricks workspace
2. Go to Workspace â†’ Import
3. Upload `.py` files from `/databricks/` folder

### Step 7: Configure Connections

Update configuration in notebooks:
```python
# Databricks notebooks
STORAGE_ACCOUNT = "dlsyourstorage"
jdbc_url = "jdbc:sqlserver://sql-dataeng-server.database.windows.net:1433;database=sales_analytics_db"
```

---

## â–¶ï¸ Running the Pipeline

### Option 1: Manual Execution (Recommended for Testing)

1. **Run Data Generation** (if not done):
   ```bash
   python sample_data/generate_sales_data.py
   python sample_data/generate_event_data.py
   ```

2. **Upload to Raw Layer** (simulate source files)

3. **Run ADF Pipeline 1**: `pipeline_ingest_sales`
   - Copies CSV â†’ Bronze layer

4. **Run ADF Pipeline 2**: `pipeline_ingest_events`
   - Copies JSON â†’ Bronze layer

5. **Run Databricks Notebook 1**: `01_bronze_to_silver.py`
   - Cleans and validates data â†’ Silver layer

6. **Run Databricks Notebook 2**: `02_silver_to_gold.py`
   - Applies business logic â†’ Gold layer + SQL DB

7. **Run ADF Pipeline 3**: `pipeline_load_table_storage`
   - Loads sessions to Azure Table Storage

### Option 2: Scheduled Execution

**Sales Pipeline**: Daily at 2 AM
```json
{
  "recurrence": {
    "frequency": "Day",
    "interval": 1,
    "schedule": { "hours": [2], "minutes": [0] }
  }
}
```

**Events Pipeline**: Every hour
```json
{
  "recurrence": {
    "frequency": "Hour",
    "interval": 1
  }
}
```

### Option 3: Local Simulation (Without Azure)

If you don't have Azure access:

1. Use **local Spark** instead of Databricks
2. Use **SQLite** instead of Azure SQL
3. Use **local filesystem** instead of Data Lake
4. Use **JSON files** instead of Table Storage

See `docs/local_setup.md` for instructions.

---

## ğŸ’¡ Design Decisions

### Why Separate Pipelines for Sales vs Events?

| Aspect | Sales | Events |
|--------|-------|--------|
| **Schema** | Fixed columns | Semi-structured JSON |
| **Volume** | 10K/day | 50K+/hour |
| **Frequency** | Daily batch | Hourly micro-batch |
| **Validation** | Strict (fail on errors) | Flexible (filter invalid) |
| **Target** | SQL DB (star schema) | Table Storage (documents) |

**Conclusion**: Different data characteristics require different processing patterns.

### Why Azure SQL Database for Sales Data?

âœ… **Structured schema** - Sales orders have fixed columns  
âœ… **ACID compliance** - Financial data needs consistency  
âœ… **Complex joins** - Analytics requires joining customers, products, dates  
âœ… **Aggregations** - Business metrics, BI reporting  
âœ… **Star schema** - Optimized for analytical queries

### Why Azure Table Storage for Event Data?

âœ… **Schema flexibility** - Event metadata varies by type  
âœ… **High write throughput** - Millions of events/day  
âœ… **Fast key lookups** - Query by user_id (partition key)  
âœ… **Cost-effective** - 90% cheaper than Cosmos DB for same workload  
âœ… **Simple data model** - No complex relationships

**Why NOT Cosmos DB?**
- âŒ Too expensive for this scale ($25-100/month vs $0.10/month)
- âŒ Overkill - Don't need global distribution or <10ms latency
- âŒ Shows poor cost awareness (important for data engineers!)

### Star Schema Design

```
Dimensions:
  - dim_customers (SCD Type 1)
  - dim_products (SCD Type 1)
  - dim_date (pre-populated)

Fact:
  - fact_sales (grain: one row per order)

Foreign Keys: customer_key, product_key, date_key
```

**Why denormalized?**
- Optimized for query performance (fewer joins)
- Analytical workload (read-heavy, not OLTP)
- Star schema is industry standard for data warehousing

---

## ğŸ›¡ï¸ Data Quality

### Validation Rules

**Sales Data**:
- âœ… `order_id` must be unique and not null
- âœ… `customer_id`, `product_id` must exist
- âœ… `quantity` > 0, `unit_price` > 0
- âœ… `line_total` >= 0
- âœ… `order_date` <= current_date
- âœ… `status` in ['completed', 'pending', 'cancelled', 'returned']

**Event Data**:
- âœ… `event_id` must be unique
- âœ… `user_id` must exist (filter out BOT%, TEST%)
- âœ… `timestamp` <= current_timestamp
- âœ… `event_type` must be valid
- âš ï¸ Metadata fields can be null (flexible)

### Error Handling

**Invalid Records**:
- Written to **quarantine tables** for analysis
- Logged in audit tables with error reason
- Never silently dropped

**Pipeline Failures**:
- **Sales**: Fail fast (financial data must be accurate)
- **Events**: Best effort (partial data OK for logs)
- Retry logic: 3 attempts with exponential backoff

### Idempotency

Pipelines can be safely rerun:
- **MERGE** statements (UPSERT) instead of INSERT
- **batch_id** tracking prevents duplicates
- Watermark tables track what's been processed
- Gold layer partitioned by date (reprocess specific dates)

---

## ğŸ“ˆ SQL Transformations

### Window Functions

**7-Day Moving Average**:
```sql
SELECT 
  order_date,
  SUM(line_total) as daily_revenue,
  AVG(SUM(line_total)) OVER (
    ORDER BY order_date 
    ROWS BETWEEN 6 PRECEDING AND CURRENT ROW
  ) as moving_avg_7day
FROM gold.fact_sales
GROUP BY order_date;
```

**Customer Ranking**:
```sql
SELECT 
  customer_name,
  total_spent,
  RANK() OVER (ORDER BY total_spent DESC) as customer_rank,
  NTILE(4) OVER (ORDER BY total_spent DESC) as quartile
FROM customer_metrics;
```

### Complex Joins

**Sales with All Dimensions**:
```sql
SELECT 
  f.order_id,
  c.customer_name,
  c.segment,
  p.product_name,
  p.category,
  d.month_name,
  d.year,
  f.line_total,
  f.profit
FROM gold.fact_sales f
LEFT JOIN gold.dim_customers c ON f.customer_key = c.customer_key
LEFT JOIN gold.dim_products p ON f.product_key = p.product_key
LEFT JOIN gold.dim_date d ON f.date_key = d.date_key
WHERE f.status = 'completed';
```

### Aggregations

**Product Performance**:
```sql
SELECT 
  category,
  COUNT(DISTINCT product_id) as product_count,
  SUM(units_sold) as total_units,
  SUM(revenue) as total_revenue,
  ROUND(100.0 * SUM(profit) / SUM(revenue), 2) as margin_pct
FROM product_metrics
GROUP BY category
ORDER BY total_revenue DESC;
```

See [SQL Transformations](sql/02_transformations.sql) for complete examples.

---

## âš ï¸ Assumptions & Limitations

### Assumptions

1. **Data Availability**: Source files arrive daily/hourly as scheduled
2. **Schema Stability**: CSV/JSON schemas don't change frequently
3. **Network**: Stable connection between Azure services
4. **Scale**: Dataset fits single Databricks cluster (not distributed)
5. **Security**: Basic authentication (not enterprise-grade RBAC)
6. **Time Zone**: All timestamps in UTC

### Limitations

1. **No Real-Time Streaming**: Batch/micro-batch only (not Event Hub/Stream Analytics)
2. **SCD Type 1 Only**: Dimensions overwrite (no history tracking)
3. **Single Region**: No geo-replication or disaster recovery
4. **Basic Error Handling**: Manual intervention required for complex failures
5. **No PII Encryption**: Sensitive data not masked/encrypted
6. **Limited Monitoring**: Basic ADF monitoring (not Azure Monitor/App Insights)
7. **Simulated Incrementals**: Real incremental loads would need CDC (Change Data Capture)

### Future Enhancements

- ğŸ”„ Implement SCD Type 2 for customer history
- ğŸ” Add data encryption and masking for PII
- ğŸ“Š Integrate with Power BI for dashboards
- âš¡ Add real-time streaming with Event Hub
- ğŸ”” Implement alerting with Azure Monitor
- ğŸ§ª Add data quality framework (Great Expectations)
- ğŸŒ Multi-region deployment with failover

---

## ğŸ“¸ Sample Outputs

### Fact Table (SQL Database)

```
sales_key | order_id  | customer_key | product_key | quantity | line_total | profit
----------|-----------|--------------|-------------|----------|------------|--------
1         | ORD000001 | 42           | 3           | 2        | 39.98      | 29.98
2         | ORD000002 | 123          | 1           | 1        | 1299.99    | 400.00
```

### Customer Metrics

```
customer_name | total_orders | total_spent | avg_order_value | customer_tier
--------------|--------------|-------------|-----------------|---------------
John Smith    | 45           | $12,543.21  | $278.74         | Top 25% (VIP)
Jane Doe      | 23           | $6,234.50   | $271.07         | Top 50% (Premium)
```

### Event Sessions (Table Storage)

```json
{
  "PartitionKey": "USER00123",
  "RowKey": "SESS123456",
  "session_start": "2024-01-15T10:30:00Z",
  "session_end": "2024-01-15T10:45:32Z",
  "duration_seconds": 932,
  "total_events": 15,
  "event_journey": [...]
}
```

See [docs/sample_outputs/](docs/sample_outputs/) for more examples.

---

## ğŸ“ Project Structure

```
/data-engineering-etl-assignment
â”‚
â”œâ”€â”€ README.md                          # This file
â”œâ”€â”€ LICENSE
â”‚
â”œâ”€â”€ /sample_data                       # Sample datasets
â”‚   â”œâ”€â”€ generate_sales_data.py         # Generates 10K sales orders
â”‚   â”œâ”€â”€ generate_event_data.py         # Generates 50K events
â”‚   â”œâ”€â”€ sales_orders.csv               # Generated sales data
â”‚   â”œâ”€â”€ customers.csv                  # Customer dimension
â”‚   â”œâ”€â”€ products.csv                   # Product dimension
â”‚   â””â”€â”€ user_events.json               # Generated events
â”‚
â”œâ”€â”€ /adf                               # Azure Data Factory pipelines
â”‚   â”œâ”€â”€ pipeline_ingest_sales.json     # Sales ingestion pipeline
â”‚   â”œâ”€â”€ pipeline_ingest_events.json    # Events ingestion pipeline
â”‚   â”œâ”€â”€ pipeline_transform_load.json   # Orchestration pipeline
â”‚   â””â”€â”€ README.md                      # ADF setup guide
â”‚
â”œâ”€â”€ /databricks                        # Databricks notebooks
â”‚   â”œâ”€â”€ 01_bronze_to_silver.py         # Data quality & cleaning
â”‚   â”œâ”€â”€ 02_silver_to_gold.py           # Business transformations
â”‚   â”œâ”€â”€ 03_advanced_analytics.py       # Window functions & analytics
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ /sql                               # SQL scripts
â”‚   â”œâ”€â”€ 01_ddl_schema.sql              # Database schema creation
â”‚   â”œâ”€â”€ 02_transformations.sql         # All transformation queries
â”‚   â”œâ”€â”€ 03_sample_queries.sql          # Analytical query examples
â”‚   â””â”€â”€ 04_audit.sql                   # Audit table setup
â”‚
â”œâ”€â”€ /docs                              # Documentation
â”‚   â”œâ”€â”€ architecture_diagram.png       # Visual architecture
â”‚   â”œâ”€â”€ design_decisions.md            # Detailed design rationale
â”‚   â”œâ”€â”€ data_quality_rules.md          # Validation rules
â”‚   â”œâ”€â”€ local_setup.md                 # Run without Azure
â”‚   â””â”€â”€ /sample_outputs                # Example outputs
â”‚
â””â”€â”€ /configs                           # Configuration files
    â”œâ”€â”€ databricks_cluster.json        # Cluster config
    â””â”€â”€ linked_services.json           # ADF connections
```

---

## ğŸ“š Additional Resources

- [Azure Data Factory Documentation](https://docs.microsoft.com/azure/data-factory/)
- [Databricks SQL Reference](https://docs.databricks.com/sql/language-manual/)
- [Medallion Architecture](https://www.databricks.com/glossary/medallion-architecture)
- [Star Schema Design](https://en.wikipedia.org/wiki/Star_schema)

---

## ğŸ“„ License

MIT License - see [LICENSE](LICENSE) file

---

## ğŸ‘¤ Author

**Your Name**  
ğŸ“§ your.email@example.com  
ğŸ”— [LinkedIn](https://linkedin.com/in/yourprofile)  
ğŸ™ [GitHub](https://github.com/yourusername)

---

## ğŸ™ Acknowledgments

- Assignment provided by [Institution/Company]
- Inspired by real-world production ETL patterns
- Sample data generated for educational purposes

---

**â­ If you found this project helpful, please give it a star!**
